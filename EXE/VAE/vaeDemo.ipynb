{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.enc_mu = torch.nn.Linear(H, latent_size)\n",
    "        self.enc_log_sigma = torch.nn.Linear(H, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mu = self.enc_mu(x)\n",
    "        log_sigma = self.enc_log_sigma(x)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return torch.distributions.Normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = torch.tanh(self.linear2(x))\n",
    "        return torch.distributions.Normal(mu, torch.ones_like(mu))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, state):\n",
    "        q_z = self.encoder(state)\n",
    "        z = q_z.rsample()\n",
    "        return self.decoder(z), q_z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     # Normalize the images to be -0.5, 0.5\n",
    "     transforms.Normalize(0.5, 1)]\n",
    "    )\n",
    "mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  60000\n"
     ]
    }
   ],
   "source": [
    "input_dim = 28 * 28\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 512\n",
    "latent_size = 8\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    mnist, batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    pin_memory=torch.cuda.is_available())\n",
    "\n",
    "print('Number of samples: ', len(mnist))\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_size, latent_size)\n",
    "decoder = Decoder(latent_size, hidden_size, input_dim)\n",
    "\n",
    "vae = VAE(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 743.0042724609375 -739.0231323242188 3.9811513423919678\n",
      "1 740.8493041992188 -735.489501953125 5.359781742095947\n",
      "2 740.7822875976562 -735.5588989257812 5.223367214202881\n",
      "3 740.4976806640625 -735.2177734375 5.279908657073975\n",
      "4 739.730712890625 -734.4281616210938 5.302572727203369\n",
      "5 739.4393920898438 -734.0872192382812 5.3521952629089355\n",
      "6 741.0579833984375 -735.5221557617188 5.535839080810547\n",
      "7 740.197509765625 -734.3346557617188 5.862834930419922\n",
      "8 740.4114379882812 -734.7138061523438 5.697638034820557\n",
      "9 739.4442749023438 -733.5923461914062 5.851914882659912\n",
      "10 739.9802856445312 -733.8739624023438 6.106298923492432\n",
      "11 739.606201171875 -734.1071166992188 5.499109745025635\n",
      "12 738.5924682617188 -732.4588012695312 6.133653163909912\n",
      "13 740.055419921875 -733.6057739257812 6.449666500091553\n",
      "14 739.4208374023438 -733.18701171875 6.233808994293213\n",
      "15 738.5640869140625 -732.84228515625 5.721797466278076\n",
      "16 740.3795776367188 -734.4822387695312 5.897331714630127\n",
      "17 739.5176391601562 -733.8119506835938 5.705702304840088\n",
      "18 739.6934814453125 -733.8764038085938 5.817060470581055\n",
      "19 740.0734252929688 -733.8770141601562 6.196441173553467\n",
      "20 739.3521728515625 -733.5515747070312 5.800577640533447\n",
      "21 739.7235107421875 -733.630126953125 6.0933685302734375\n",
      "22 739.1639404296875 -732.881591796875 6.28236722946167\n",
      "23 738.9163818359375 -733.0604858398438 5.855902194976807\n",
      "24 739.8301391601562 -733.7659301757812 6.064201831817627\n",
      "25 739.6859741210938 -733.4658203125 6.220130443572998\n",
      "26 739.4401245117188 -733.478271484375 5.9618821144104\n",
      "27 738.991943359375 -732.9609375 6.031035900115967\n",
      "28 739.8656005859375 -733.5408325195312 6.324761867523193\n",
      "29 739.3958740234375 -733.263916015625 6.131960391998291\n",
      "30 739.8050537109375 -733.8788452148438 5.926204204559326\n",
      "31 739.8330078125 -733.91064453125 5.922386169433594\n",
      "32 738.5885009765625 -732.5233764648438 6.0651021003723145\n",
      "33 738.1087646484375 -732.146728515625 5.962013244628906\n",
      "34 740.20263671875 -733.974365234375 6.228260517120361\n",
      "35 739.1910400390625 -732.9948120117188 6.196238040924072\n",
      "36 739.7472534179688 -733.4511108398438 6.296117305755615\n",
      "37 738.7242431640625 -732.4534301757812 6.27080774307251\n",
      "38 739.54736328125 -733.18798828125 6.359386444091797\n",
      "39 739.45947265625 -732.729736328125 6.729715347290039\n",
      "40 739.4129638671875 -733.2592163085938 6.1537322998046875\n",
      "41 739.6663818359375 -733.457275390625 6.209075927734375\n",
      "42 738.8120727539062 -732.5702514648438 6.241802215576172\n",
      "43 739.5133056640625 -733.338623046875 6.174688816070557\n",
      "44 739.4053955078125 -733.5094604492188 5.895912170410156\n",
      "45 740.114013671875 -733.5078125 6.606173038482666\n",
      "46 738.8582763671875 -732.7355346679688 6.1227240562438965\n",
      "47 739.3468627929688 -732.8973999023438 6.449460506439209\n",
      "48 739.3685913085938 -733.0614624023438 6.307106018066406\n",
      "49 738.8626098632812 -732.60546875 6.25713586807251\n",
      "50 739.2472534179688 -733.16064453125 6.086605548858643\n",
      "51 739.5543823242188 -733.204833984375 6.3495354652404785\n",
      "52 739.1964111328125 -733.2798461914062 5.916537761688232\n",
      "53 739.5789794921875 -733.099853515625 6.479135990142822\n",
      "54 739.265625 -733.0643920898438 6.201222896575928\n",
      "55 737.8551025390625 -731.84326171875 6.011865615844727\n",
      "56 739.26708984375 -732.713623046875 6.553478240966797\n",
      "57 738.9380493164062 -732.4672241210938 6.4708123207092285\n",
      "58 738.2813720703125 -731.840576171875 6.440820217132568\n",
      "59 738.99609375 -733.0625 5.933574676513672\n",
      "60 739.12109375 -732.7587280273438 6.362362384796143\n",
      "61 740.127197265625 -733.7352905273438 6.3919243812561035\n",
      "62 738.4169921875 -732.45458984375 5.962427616119385\n",
      "63 739.5480346679688 -733.3346557617188 6.213386535644531\n",
      "64 739.0499877929688 -732.628173828125 6.4217915534973145\n",
      "65 738.6433715820312 -732.4381713867188 6.205188751220703\n",
      "66 739.313232421875 -732.8405151367188 6.472745418548584\n",
      "67 739.7540893554688 -733.4895629882812 6.264509677886963\n",
      "68 738.9923706054688 -732.6270141601562 6.365330219268799\n",
      "69 738.5565185546875 -732.2031860351562 6.35333251953125\n",
      "70 739.1624145507812 -733.0535278320312 6.108870983123779\n",
      "71 739.3067626953125 -732.94091796875 6.365823268890381\n",
      "72 739.2083740234375 -732.8609008789062 6.347484111785889\n",
      "73 740.1041259765625 -733.6272583007812 6.476875305175781\n",
      "74 739.4244995117188 -733.3645629882812 6.05994987487793\n",
      "75 739.3426513671875 -732.9462890625 6.396360397338867\n",
      "76 739.4974365234375 -733.1624145507812 6.335018157958984\n",
      "77 739.8307495117188 -733.4009399414062 6.429798603057861\n",
      "78 740.0330200195312 -733.5494995117188 6.48352575302124\n",
      "79 738.4959716796875 -732.2311401367188 6.264833927154541\n",
      "80 738.9791259765625 -732.4559936523438 6.523157119750977\n",
      "81 739.7160034179688 -733.06396484375 6.652065277099609\n",
      "82 739.8563232421875 -733.3154907226562 6.540833950042725\n",
      "83 739.1190185546875 -732.4100952148438 6.708933353424072\n",
      "84 739.4189453125 -732.9136352539062 6.505338668823242\n",
      "85 739.810302734375 -733.3715209960938 6.4387664794921875\n",
      "86 738.922607421875 -732.7583618164062 6.164233684539795\n",
      "87 738.6876831054688 -732.4791870117188 6.208492279052734\n",
      "88 739.3817138671875 -733.2481079101562 6.133613586425781\n",
      "89 739.377685546875 -732.9684448242188 6.409242630004883\n",
      "90 739.4337768554688 -733.04833984375 6.385438919067383\n",
      "91 739.5925903320312 -733.2902221679688 6.302381992340088\n",
      "92 739.54052734375 -733.324951171875 6.215576171875\n",
      "93 738.6354370117188 -732.4677124023438 6.16769552230835\n",
      "94 739.3790283203125 -733.2213745117188 6.157632350921631\n",
      "95 738.4926147460938 -732.2824096679688 6.210201263427734\n",
      "96 739.8975830078125 -733.3331909179688 6.564413547515869\n",
      "97 738.8931274414062 -732.5482788085938 6.344875812530518\n",
      "98 738.565673828125 -732.2692260742188 6.296453475952148\n",
      "99 739.9083251953125 -733.33447265625 6.573877334594727\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.view(-1, input_dim).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        p_x, q_z = vae(inputs)\n",
    "        log_likelihood = p_x.log_prob(inputs).sum(-1).mean()\n",
    "        kl = torch.distributions.kl_divergence(\n",
    "            q_z, \n",
    "            torch.distributions.Normal(0, 1.)\n",
    "        ).sum(-1).mean()\n",
    "        loss = -(log_likelihood - kl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l = loss.item()\n",
    "    print(epoch, l, log_likelihood.item(), kl.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForEXE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
