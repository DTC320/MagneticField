{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "\n",
    "        # 卷积层\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(32 * 8 * 8, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 3 * 32 * 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "wandb.init(project=\"AE_MNIST\")\n",
    "# 实例化网络\n",
    "model = DiffusionModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 加载数据并划分为训练集、验证集和测试集\n",
    "data = np.load(\"data4D.npy\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 将NumPy数组转换为张量\n",
    "train_data_tensor = torch.from_numpy(train_data)\n",
    "val_data_tensor = torch.from_numpy(val_data)\n",
    "test_data_tensor = torch.from_numpy(test_data)\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(train_data_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_data_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "reg_coeff = 0.001  # L2正则化系数\n",
    "\n",
    "def calc_l2_reg(model):\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.norm(param, p=2)\n",
    "    return l2_reg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [109/500], Training Loss: 0.12748829036951065\n",
      "Epoch [109/500], Validation Loss: 0.03614046850374767\n",
      "Epoch [110/500], Training Loss: 0.12754880011081696\n",
      "Epoch [110/500], Validation Loss: 0.03602374504719462\n",
      "Epoch [111/500], Training Loss: 0.12781387388706208\n",
      "Epoch [111/500], Validation Loss: 0.03605290928057262\n",
      "Epoch [112/500], Training Loss: 0.12811190664768218\n",
      "Epoch [112/500], Validation Loss: 0.03623951545783451\n",
      "Epoch [113/500], Training Loss: 0.12818334996700287\n",
      "Epoch [113/500], Validation Loss: 0.03598384452717645\n",
      "Epoch [114/500], Training Loss: 0.1283734595775604\n",
      "Epoch [114/500], Validation Loss: 0.03594383863466127\n",
      "Epoch [115/500], Training Loss: 0.1285304307937622\n",
      "Epoch [115/500], Validation Loss: 0.03588454691427095\n",
      "Epoch [116/500], Training Loss: 0.128747096657753\n",
      "Epoch [116/500], Validation Loss: 0.03593392883028303\n",
      "Epoch [117/500], Training Loss: 0.1288880681991577\n",
      "Epoch [117/500], Validation Loss: 0.03596841810005052\n",
      "Epoch [118/500], Training Loss: 0.12904146194458008\n",
      "Epoch [118/500], Validation Loss: 0.03591218218207359\n",
      "Epoch [119/500], Training Loss: 0.12935176640748977\n",
      "Epoch [119/500], Validation Loss: 0.0359809191099235\n",
      "Epoch [120/500], Training Loss: 0.1294780719280243\n",
      "Epoch [120/500], Validation Loss: 0.035893907504422326\n",
      "Epoch [121/500], Training Loss: 0.12955567598342896\n",
      "Epoch [121/500], Validation Loss: 0.035946138203144073\n",
      "Epoch [122/500], Training Loss: 0.12980180859565735\n",
      "Epoch [122/500], Validation Loss: 0.03596600517630577\n",
      "Epoch [123/500], Training Loss: 0.1300046283006668\n",
      "Epoch [123/500], Validation Loss: 0.035891414220844\n",
      "Epoch [124/500], Training Loss: 0.13032436788082122\n",
      "Epoch [124/500], Validation Loss: 0.03602200640099389\n",
      "Epoch [125/500], Training Loss: 0.1303848147392273\n",
      "Epoch [125/500], Validation Loss: 0.035860338381358554\n",
      "Epoch [126/500], Training Loss: 0.13054311752319336\n",
      "Epoch [126/500], Validation Loss: 0.03590499237179756\n",
      "Epoch [127/500], Training Loss: 0.13070095896720887\n",
      "Epoch [127/500], Validation Loss: 0.035998061299324036\n",
      "Epoch [128/500], Training Loss: 0.13105956912040712\n",
      "Epoch [128/500], Validation Loss: 0.035963388425963264\n",
      "Epoch [129/500], Training Loss: 0.13111596465110778\n",
      "Epoch [129/500], Validation Loss: 0.03593355257596288\n",
      "Epoch [130/500], Training Loss: 0.1312612533569336\n",
      "Epoch [130/500], Validation Loss: 0.03586457243987492\n",
      "Epoch [131/500], Training Loss: 0.13145761251449584\n",
      "Epoch [131/500], Validation Loss: 0.03597649399723325\n",
      "Epoch [132/500], Training Loss: 0.13175499737262725\n",
      "Epoch [132/500], Validation Loss: 0.035900471465928216\n",
      "Epoch [133/500], Training Loss: 0.1318648040294647\n",
      "Epoch [133/500], Validation Loss: 0.03585410863161087\n",
      "Epoch [134/500], Training Loss: 0.132005175948143\n",
      "Epoch [134/500], Validation Loss: 0.03588203979390008\n",
      "Epoch [135/500], Training Loss: 0.13222578227519988\n",
      "Epoch [135/500], Validation Loss: 0.03592045126216752\n",
      "Epoch [136/500], Training Loss: 0.1324061381816864\n",
      "Epoch [136/500], Validation Loss: 0.035933922444071086\n",
      "Epoch [137/500], Training Loss: 0.13266533970832825\n",
      "Epoch [137/500], Validation Loss: 0.03594872515116419\n",
      "Epoch [138/500], Training Loss: 0.13280828535556793\n",
      "Epoch [138/500], Validation Loss: 0.03587757210646357\n",
      "Epoch [139/500], Training Loss: 0.1330404418706894\n",
      "Epoch [139/500], Validation Loss: 0.035943612456321716\n",
      "Epoch [140/500], Training Loss: 0.13320256292819976\n",
      "Epoch [140/500], Validation Loss: 0.035880150007350106\n",
      "Epoch [141/500], Training Loss: 0.13340542376041412\n",
      "Epoch [141/500], Validation Loss: 0.035948200417416434\n",
      "Epoch [142/500], Training Loss: 0.13358632504940032\n",
      "Epoch [142/500], Validation Loss: 0.035843800753355026\n",
      "Epoch [143/500], Training Loss: 0.13369157314300537\n",
      "Epoch [143/500], Validation Loss: 0.03575937130621502\n",
      "Epoch [144/500], Training Loss: 0.13382963120937347\n",
      "Epoch [144/500], Validation Loss: 0.035825292978967936\n",
      "Epoch [145/500], Training Loss: 0.13409575521945954\n",
      "Epoch [145/500], Validation Loss: 0.03584064436810357\n",
      "Epoch [146/500], Training Loss: 0.1343344831466675\n",
      "Epoch [146/500], Validation Loss: 0.03581634749259267\n",
      "Epoch [147/500], Training Loss: 0.13464702248573304\n",
      "Epoch [147/500], Validation Loss: 0.03584324728165354\n",
      "Epoch [148/500], Training Loss: 0.1347085279226303\n",
      "Epoch [148/500], Validation Loss: 0.03578717846955572\n",
      "Epoch [149/500], Training Loss: 0.13488352835178374\n",
      "Epoch [149/500], Validation Loss: 0.03594552778771946\n",
      "Epoch [150/500], Training Loss: 0.1351046395301819\n",
      "Epoch [150/500], Validation Loss: 0.035759429314306805\n",
      "Epoch [151/500], Training Loss: 0.13516970098018646\n",
      "Epoch [151/500], Validation Loss: 0.03571529846106257\n",
      "Epoch [152/500], Training Loss: 0.13537976622581482\n",
      "Epoch [152/500], Validation Loss: 0.03585032852632659\n",
      "Epoch [153/500], Training Loss: 0.13560514509677887\n",
      "Epoch [153/500], Validation Loss: 0.03571824889097895\n",
      "Epoch [154/500], Training Loss: 0.13573522806167604\n",
      "Epoch [154/500], Validation Loss: 0.03578501141497067\n",
      "Epoch [155/500], Training Loss: 0.1359045284986496\n",
      "Epoch [155/500], Validation Loss: 0.035802999777453284\n",
      "Epoch [156/500], Training Loss: 0.13610292315483094\n",
      "Epoch [156/500], Validation Loss: 0.035712365593229024\n",
      "Epoch [157/500], Training Loss: 0.1362254500389099\n",
      "Epoch [157/500], Validation Loss: 0.03581344389489719\n",
      "Epoch [158/500], Training Loss: 0.1364462798833847\n",
      "Epoch [158/500], Validation Loss: 0.035749568470886776\n",
      "Epoch [159/500], Training Loss: 0.1365992820262909\n",
      "Epoch [159/500], Validation Loss: 0.035701761820486615\n",
      "Epoch [160/500], Training Loss: 0.13677677989006043\n",
      "Epoch [160/500], Validation Loss: 0.035743887935365946\n",
      "Epoch [161/500], Training Loss: 0.13724031507968903\n",
      "Epoch [161/500], Validation Loss: 0.035866751202515194\n",
      "Epoch [162/500], Training Loss: 0.13726130962371827\n",
      "Epoch [162/500], Validation Loss: 0.03570263300623212\n",
      "Epoch [163/500], Training Loss: 0.13739315450191497\n",
      "Epoch [163/500], Validation Loss: 0.03565674860562597\n",
      "Epoch [164/500], Training Loss: 0.13760241627693176\n",
      "Epoch [164/500], Validation Loss: 0.0357764262173857\n",
      "Epoch [165/500], Training Loss: 0.13781962752342225\n",
      "Epoch [165/500], Validation Loss: 0.0358580547784056\n",
      "Epoch [166/500], Training Loss: 0.138026362657547\n",
      "Epoch [166/500], Validation Loss: 0.03567146509885788\n",
      "Epoch [167/500], Training Loss: 0.13823205411434172\n",
      "Epoch [167/500], Validation Loss: 0.035692007946116586\n",
      "Epoch [168/500], Training Loss: 0.13841766476631165\n",
      "Epoch [168/500], Validation Loss: 0.03568742051720619\n",
      "Epoch [169/500], Training Loss: 0.13847363293170928\n",
      "Epoch [169/500], Validation Loss: 0.03582142825637545\n",
      "Epoch [170/500], Training Loss: 0.13878576576709747\n",
      "Epoch [170/500], Validation Loss: 0.03575401061347553\n",
      "Epoch [171/500], Training Loss: 0.1389363771677017\n",
      "Epoch [171/500], Validation Loss: 0.035745671817234585\n",
      "Epoch [172/500], Training Loss: 0.13909244477748872\n",
      "Epoch [172/500], Validation Loss: 0.035676341503858566\n",
      "Epoch [173/500], Training Loss: 0.13916686952114105\n",
      "Epoch [173/500], Validation Loss: 0.03557688955749784\n",
      "Epoch [174/500], Training Loss: 0.13937753081321716\n",
      "Epoch [174/500], Validation Loss: 0.035743954458406994\n",
      "Epoch [175/500], Training Loss: 0.13965845227241516\n",
      "Epoch [175/500], Validation Loss: 0.03580218500324658\n",
      "Epoch [176/500], Training Loss: 0.13978771328926087\n",
      "Epoch [176/500], Validation Loss: 0.035661857042993815\n",
      "Epoch [177/500], Training Loss: 0.13994916200637816\n",
      "Epoch [177/500], Validation Loss: 0.03564692661166191\n",
      "Epoch [178/500], Training Loss: 0.1401574957370758\n",
      "Epoch [178/500], Validation Loss: 0.03559585926788194\n",
      "Epoch [179/500], Training Loss: 0.1403811478614807\n",
      "Epoch [179/500], Validation Loss: 0.03567897102662495\n",
      "Epoch [180/500], Training Loss: 0.1405324673652649\n",
      "Epoch [180/500], Validation Loss: 0.035679069480725696\n",
      "Epoch [181/500], Training Loss: 0.14071562469005586\n",
      "Epoch [181/500], Validation Loss: 0.035603586584329605\n",
      "Epoch [182/500], Training Loss: 0.14090600728988648\n",
      "Epoch [182/500], Validation Loss: 0.03570306726864406\n",
      "Epoch [183/500], Training Loss: 0.14118904650211334\n",
      "Epoch [183/500], Validation Loss: 0.035823392548731396\n",
      "Epoch [184/500], Training Loss: 0.14133055746555329\n",
      "Epoch [184/500], Validation Loss: 0.03562684463603156\n",
      "Epoch [185/500], Training Loss: 0.1416032838821411\n",
      "Epoch [185/500], Validation Loss: 0.03564520073788507\n",
      "Epoch [186/500], Training Loss: 0.14180114150047302\n",
      "Epoch [186/500], Validation Loss: 0.0356180194233145\n",
      "Epoch [187/500], Training Loss: 0.14193661510944366\n",
      "Epoch [187/500], Validation Loss: 0.03565635106393269\n",
      "Epoch [188/500], Training Loss: 0.1421023589372635\n",
      "Epoch [188/500], Validation Loss: 0.03567839254226003\n",
      "Epoch [189/500], Training Loss: 0.14224193334579469\n",
      "Epoch [189/500], Validation Loss: 0.035559426460947306\n",
      "Epoch [190/500], Training Loss: 0.14236547887325288\n",
      "Epoch [190/500], Validation Loss: 0.03560416506869452\n",
      "Epoch [191/500], Training Loss: 0.14254321157932281\n",
      "Epoch [191/500], Validation Loss: 0.035501698298113685\n",
      "Epoch [192/500], Training Loss: 0.14263283729553222\n",
      "Epoch [192/500], Validation Loss: 0.03550627189023154\n",
      "Epoch [193/500], Training Loss: 0.14280454635620118\n",
      "Epoch [193/500], Validation Loss: 0.03552502393722534\n",
      "Epoch [194/500], Training Loss: 0.14293156385421754\n",
      "Epoch [194/500], Validation Loss: 0.03550507873296738\n",
      "Epoch [195/500], Training Loss: 0.14311262786388398\n",
      "Epoch [195/500], Validation Loss: 0.0355714676635606\n",
      "Epoch [196/500], Training Loss: 0.14331264793872833\n",
      "Epoch [196/500], Validation Loss: 0.03558576213462012\n",
      "Epoch [197/500], Training Loss: 0.14351813554763793\n",
      "Epoch [197/500], Validation Loss: 0.03551797728453364\n",
      "Epoch [198/500], Training Loss: 0.14363231360912324\n",
      "Epoch [198/500], Validation Loss: 0.03559793212584087\n",
      "Epoch [199/500], Training Loss: 0.14388251304626465\n",
      "Epoch [199/500], Validation Loss: 0.035640974662133625\n",
      "Epoch [200/500], Training Loss: 0.14403016984462738\n",
      "Epoch [200/500], Validation Loss: 0.03564932516642979\n",
      "Epoch [201/500], Training Loss: 0.14423640072345734\n",
      "Epoch [201/500], Validation Loss: 0.03554833201425416\n",
      "Epoch [202/500], Training Loss: 0.1444875431060791\n",
      "Epoch [202/500], Validation Loss: 0.03568116309387343\n",
      "Epoch [203/500], Training Loss: 0.14461585879325867\n",
      "Epoch [203/500], Validation Loss: 0.035525246390274594\n",
      "Epoch [204/500], Training Loss: 0.14479828596115113\n",
      "Epoch [204/500], Validation Loss: 0.035750107041427066\n",
      "Epoch [205/500], Training Loss: 0.1450602036714554\n",
      "Epoch [205/500], Validation Loss: 0.03552003524133137\n",
      "Epoch [206/500], Training Loss: 0.14517794609069823\n",
      "Epoch [206/500], Validation Loss: 0.03578017811690058\n",
      "Epoch [207/500], Training Loss: 0.14534722924232482\n",
      "Epoch [207/500], Validation Loss: 0.035602446645498276\n",
      "Epoch [208/500], Training Loss: 0.1455019861459732\n",
      "Epoch [208/500], Validation Loss: 0.03558314964175224\n",
      "Epoch [209/500], Training Loss: 0.1456983596086502\n",
      "Epoch [209/500], Validation Loss: 0.03557769847767694\n",
      "Epoch [210/500], Training Loss: 0.14581992745399475\n",
      "Epoch [210/500], Validation Loss: 0.03568180331162044\n",
      "Epoch [211/500], Training Loss: 0.14601796627044678\n",
      "Epoch [211/500], Validation Loss: 0.03550365354333605\n",
      "Epoch [212/500], Training Loss: 0.1462148219347\n",
      "Epoch [212/500], Validation Loss: 0.035713681152888706\n",
      "Epoch [213/500], Training Loss: 0.14641909897327424\n",
      "Epoch [213/500], Validation Loss: 0.03555378903235708\n",
      "Epoch [214/500], Training Loss: 0.14652719736099243\n",
      "Epoch [214/500], Validation Loss: 0.0355224891432694\n",
      "Epoch [215/500], Training Loss: 0.14670271158218384\n",
      "Epoch [215/500], Validation Loss: 0.035574015762124746\n",
      "Epoch [216/500], Training Loss: 0.14691202640533446\n",
      "Epoch [216/500], Validation Loss: 0.035612959414720535\n",
      "Epoch [217/500], Training Loss: 0.1470858347415924\n",
      "Epoch [217/500], Validation Loss: 0.035541055990116935\n",
      "Epoch [218/500], Training Loss: 0.14721455097198485\n",
      "Epoch [218/500], Validation Loss: 0.03557873251182692\n",
      "Epoch [219/500], Training Loss: 0.14739021718502043\n",
      "Epoch [219/500], Validation Loss: 0.035552871014390676\n",
      "Epoch [220/500], Training Loss: 0.1476647412776947\n",
      "Epoch [220/500], Validation Loss: 0.035822405346802304\n",
      "Epoch [221/500], Training Loss: 0.14787620127201082\n",
      "Epoch [221/500], Validation Loss: 0.03566251056534903\n",
      "Epoch [222/500], Training Loss: 0.14802102208137513\n",
      "Epoch [222/500], Validation Loss: 0.03551377888236727\n",
      "Epoch [223/500], Training Loss: 0.1481318926811218\n",
      "Epoch [223/500], Validation Loss: 0.03550762204187257\n",
      "Epoch [224/500], Training Loss: 0.1482977467775345\n",
      "Epoch [224/500], Validation Loss: 0.035496209881135395\n",
      "Epoch [225/500], Training Loss: 0.14846448242664337\n",
      "Epoch [225/500], Validation Loss: 0.035467522484915595\n",
      "Epoch [226/500], Training Loss: 0.14866462707519532\n",
      "Epoch [226/500], Validation Loss: 0.03544518085462706\n",
      "Epoch [227/500], Training Loss: 0.1488056993484497\n",
      "Epoch [227/500], Validation Loss: 0.0354293238903795\n",
      "Epoch [228/500], Training Loss: 0.14895296812057496\n",
      "Epoch [228/500], Validation Loss: 0.03538921368973596\n",
      "Epoch [229/500], Training Loss: 0.14909359872341155\n",
      "Epoch [229/500], Validation Loss: 0.035532639494964054\n",
      "Epoch [230/500], Training Loss: 0.14936727702617644\n",
      "Epoch [230/500], Validation Loss: 0.0354453335915293\n",
      "Epoch [231/500], Training Loss: 0.14942139029502868\n",
      "Epoch [231/500], Validation Loss: 0.03540431388786861\n",
      "Epoch [232/500], Training Loss: 0.14963944971561433\n",
      "Epoch [232/500], Validation Loss: 0.03543686174920627\n",
      "Epoch [233/500], Training Loss: 0.14982253670692444\n",
      "Epoch [233/500], Validation Loss: 0.03553408969725881\n",
      "Epoch [234/500], Training Loss: 0.14994428753852845\n",
      "Epoch [234/500], Validation Loss: 0.03534145014626639\n",
      "Epoch [235/500], Training Loss: 0.15013961970806122\n",
      "Epoch [235/500], Validation Loss: 0.035427034965583255\n",
      "Epoch [236/500], Training Loss: 0.1503925621509552\n",
      "Epoch [236/500], Validation Loss: 0.03557129257491657\n",
      "Epoch [237/500], Training Loss: 0.1505189770460129\n",
      "Epoch [237/500], Validation Loss: 0.03542469282235418\n",
      "Epoch [238/500], Training Loss: 0.15069205045700074\n",
      "Epoch [238/500], Validation Loss: 0.03536173754504749\n",
      "Epoch [239/500], Training Loss: 0.1508207982778549\n",
      "Epoch [239/500], Validation Loss: 0.03544475297842707\n",
      "Epoch [240/500], Training Loss: 0.15095906376838683\n",
      "Epoch [240/500], Validation Loss: 0.03548898601106235\n",
      "Epoch [241/500], Training Loss: 0.1510833704471588\n",
      "Epoch [241/500], Validation Loss: 0.035360598138400486\n",
      "Epoch [242/500], Training Loss: 0.15128344893455506\n",
      "Epoch [242/500], Validation Loss: 0.03539100821529116\n",
      "Epoch [243/500], Training Loss: 0.15140972554683685\n",
      "Epoch [243/500], Validation Loss: 0.03539327159523964\n",
      "Epoch [244/500], Training Loss: 0.15167248547077178\n",
      "Epoch [244/500], Validation Loss: 0.035435225282396586\n",
      "Epoch [245/500], Training Loss: 0.15183767557144165\n",
      "Epoch [245/500], Validation Loss: 0.03539553497518812\n",
      "Epoch [246/500], Training Loss: 0.15200588166713713\n",
      "Epoch [246/500], Validation Loss: 0.03551964887550899\n",
      "Epoch [247/500], Training Loss: 0.15218615055084228\n",
      "Epoch [247/500], Validation Loss: 0.03538166465503829\n",
      "Epoch [248/500], Training Loss: 0.1523367100954056\n",
      "Epoch [248/500], Validation Loss: 0.035451311618089676\n",
      "Epoch [249/500], Training Loss: 0.15259473383426667\n",
      "Epoch [249/500], Validation Loss: 0.03541962483099529\n",
      "Epoch [250/500], Training Loss: 0.15276579737663268\n",
      "Epoch [250/500], Validation Loss: 0.03531768971255848\n",
      "Epoch [251/500], Training Loss: 0.15293843448162078\n",
      "Epoch [251/500], Validation Loss: 0.035350083240440915\n",
      "Epoch [252/500], Training Loss: 0.15315237522125244\n",
      "Epoch [252/500], Validation Loss: 0.03539111518434116\n",
      "Epoch [253/500], Training Loss: 0.15325004279613494\n",
      "Epoch [253/500], Validation Loss: 0.03538493013807705\n",
      "Epoch [254/500], Training Loss: 0.1534385347366333\n",
      "Epoch [254/500], Validation Loss: 0.035382779581206183\n",
      "Epoch [255/500], Training Loss: 0.1535880845785141\n",
      "Epoch [255/500], Validation Loss: 0.035312852157013755\n",
      "Epoch [256/500], Training Loss: 0.1538240933418274\n",
      "Epoch [256/500], Validation Loss: 0.0353463494351932\n",
      "Epoch [257/500], Training Loss: 0.15402416348457337\n",
      "Epoch [257/500], Validation Loss: 0.03540518241269248\n",
      "Epoch [258/500], Training Loss: 0.15417239606380462\n",
      "Epoch [258/500], Validation Loss: 0.03531365469098091\n",
      "Epoch [259/500], Training Loss: 0.1543191182613373\n",
      "Epoch [259/500], Validation Loss: 0.03542386580790792\n",
      "Epoch [260/500], Training Loss: 0.15449024260044097\n",
      "Epoch [260/500], Validation Loss: 0.03534467252237456\n",
      "Epoch [261/500], Training Loss: 0.154671089053154\n",
      "Epoch [261/500], Validation Loss: 0.03530430740543774\n",
      "Epoch [262/500], Training Loss: 0.15483681797981264\n",
      "Epoch [262/500], Validation Loss: 0.03538900879876954\n",
      "Epoch [263/500], Training Loss: 0.15494852125644684\n",
      "Epoch [263/500], Validation Loss: 0.035434704806123464\n",
      "Epoch [264/500], Training Loss: 0.1550776642560959\n",
      "Epoch [264/500], Validation Loss: 0.03534163587859699\n",
      "Epoch [265/500], Training Loss: 0.15524029850959778\n",
      "Epoch [265/500], Validation Loss: 0.035436347659145086\n",
      "Epoch [266/500], Training Loss: 0.15542065620422363\n",
      "Epoch [266/500], Validation Loss: 0.03528291838509696\n",
      "Epoch [267/500], Training Loss: 0.15555579006671905\n",
      "Epoch [267/500], Validation Loss: 0.035297131431954246\n",
      "Epoch [268/500], Training Loss: 0.1558253872394562\n",
      "Epoch [268/500], Validation Loss: 0.03536505252122879\n",
      "Epoch [269/500], Training Loss: 0.15598513662815094\n",
      "Epoch [269/500], Validation Loss: 0.03533736243844032\n",
      "Epoch [270/500], Training Loss: 0.15619217932224275\n",
      "Epoch [270/500], Validation Loss: 0.035320705601147244\n",
      "Epoch [271/500], Training Loss: 0.15630923926830292\n",
      "Epoch [271/500], Validation Loss: 0.035326478736741204\n",
      "Epoch [272/500], Training Loss: 0.15647762835025789\n",
      "Epoch [272/500], Validation Loss: 0.03523446779165949\n",
      "Epoch [273/500], Training Loss: 0.15660076320171357\n",
      "Epoch [273/500], Validation Loss: 0.035221662904535024\n",
      "Epoch [274/500], Training Loss: 0.15677692890167236\n",
      "Epoch [274/500], Validation Loss: 0.035395504640681405\n",
      "Epoch [275/500], Training Loss: 0.15691876471042632\n",
      "Epoch [275/500], Validation Loss: 0.03520248777100018\n",
      "Epoch [276/500], Training Loss: 0.15707415997982024\n",
      "Epoch [276/500], Validation Loss: 0.035220835890088766\n",
      "Epoch [277/500], Training Loss: 0.15721733450889588\n",
      "Epoch [277/500], Validation Loss: 0.03524498588272503\n",
      "Epoch [278/500], Training Loss: 0.157435559630394\n",
      "Epoch [278/500], Validation Loss: 0.035241665584700446\n",
      "Epoch [279/500], Training Loss: 0.15759490430355072\n",
      "Epoch [279/500], Validation Loss: 0.035214390073503764\n",
      "Epoch [280/500], Training Loss: 0.1577463036775589\n",
      "Epoch [280/500], Validation Loss: 0.03531130137188094\n",
      "Epoch [281/500], Training Loss: 0.15787604987621306\n",
      "Epoch [281/500], Validation Loss: 0.03514303745968001\n",
      "Epoch [282/500], Training Loss: 0.1580554610490799\n",
      "Epoch [282/500], Validation Loss: 0.0353864851806845\n",
      "Epoch [283/500], Training Loss: 0.1581920152902603\n",
      "Epoch [283/500], Validation Loss: 0.03520679527095386\n",
      "Epoch [284/500], Training Loss: 0.15838839948177338\n",
      "Epoch [284/500], Validation Loss: 0.035219643265008926\n",
      "Epoch [285/500], Training Loss: 0.15850480198860167\n",
      "Epoch [285/500], Validation Loss: 0.03520274002637182\n",
      "Epoch [286/500], Training Loss: 0.1587197560071945\n",
      "Epoch [286/500], Validation Loss: 0.03527328797749111\n",
      "Epoch [287/500], Training Loss: 0.1588909661769867\n",
      "Epoch [287/500], Validation Loss: 0.03525817021727562\n",
      "Epoch [288/500], Training Loss: 0.15928586602210998\n",
      "Epoch [288/500], Validation Loss: 0.0352818529520716\n",
      "Epoch [289/500], Training Loss: 0.15931507766246797\n",
      "Epoch [289/500], Validation Loss: 0.035249798957790644\n",
      "Epoch [290/500], Training Loss: 0.15945271730422975\n",
      "Epoch [290/500], Validation Loss: 0.035269743629864285\n",
      "Epoch [291/500], Training Loss: 0.1597907727956772\n",
      "Epoch [291/500], Validation Loss: 0.0352645931499345\n",
      "Epoch [292/500], Training Loss: 0.15993598759174346\n",
      "Epoch [292/500], Validation Loss: 0.03520353351320539\n",
      "Epoch [293/500], Training Loss: 0.16003286242485046\n",
      "Epoch [293/500], Validation Loss: 0.03524934819766453\n",
      "Epoch [294/500], Training Loss: 0.16031831443309785\n",
      "Epoch [294/500], Validation Loss: 0.03531596862844059\n",
      "Epoch [295/500], Training Loss: 0.16050152242183685\n",
      "Epoch [295/500], Validation Loss: 0.035555116300071986\n",
      "Epoch [296/500], Training Loss: 0.16065892934799195\n",
      "Epoch [296/500], Validation Loss: 0.03523565296615873\n",
      "Epoch [297/500], Training Loss: 0.16072169005870818\n",
      "Epoch [297/500], Validation Loss: 0.03508928684251649\n",
      "Epoch [298/500], Training Loss: 0.1608748745918274\n",
      "Epoch [298/500], Validation Loss: 0.0351163957800184\n",
      "Epoch [299/500], Training Loss: 0.16099027633666993\n",
      "Epoch [299/500], Validation Loss: 0.03515392116137913\n",
      "Epoch [300/500], Training Loss: 0.16112165927886962\n",
      "Epoch [300/500], Validation Loss: 0.03514277668935912\n",
      "Epoch [301/500], Training Loss: 0.1612992024421692\n",
      "Epoch [301/500], Validation Loss: 0.03525414424283164\n",
      "Epoch [302/500], Training Loss: 0.16147276520729065\n",
      "Epoch [302/500], Validation Loss: 0.03529931338770049\n",
      "Epoch [303/500], Training Loss: 0.16163642704486847\n",
      "Epoch [303/500], Validation Loss: 0.035219213260071616\n",
      "Epoch [304/500], Training Loss: 0.1617862468957901\n",
      "Epoch [304/500], Validation Loss: 0.035290299249546866\n",
      "Epoch [305/500], Training Loss: 0.1619993841648102\n",
      "Epoch [305/500], Validation Loss: 0.035238203193460195\n",
      "Epoch [306/500], Training Loss: 0.16216919362545013\n",
      "Epoch [306/500], Validation Loss: 0.0351577273436955\n",
      "Epoch [307/500], Training Loss: 0.16229088127613067\n",
      "Epoch [307/500], Validation Loss: 0.03506983124784061\n",
      "Epoch [308/500], Training Loss: 0.16243400156497956\n",
      "Epoch [308/500], Validation Loss: 0.03508474198835237\n",
      "Epoch [309/500], Training Loss: 0.16264387905597688\n",
      "Epoch [309/500], Validation Loss: 0.03515056414263589\n",
      "Epoch [310/500], Training Loss: 0.16278526186943054\n",
      "Epoch [310/500], Validation Loss: 0.035110111215284893\n",
      "Epoch [311/500], Training Loss: 0.16302004396915437\n",
      "Epoch [311/500], Validation Loss: 0.03517267533711025\n",
      "Epoch [312/500], Training Loss: 0.16317394852638245\n",
      "Epoch [312/500], Validation Loss: 0.03515416117651122\n",
      "Epoch [313/500], Training Loss: 0.16325096189975738\n",
      "Epoch [313/500], Validation Loss: 0.035057300967829566\n",
      "Epoch [314/500], Training Loss: 0.16339704453945159\n",
      "Epoch [314/500], Validation Loss: 0.03507443198135921\n",
      "Epoch [315/500], Training Loss: 0.16355641365051268\n",
      "Epoch [315/500], Validation Loss: 0.035142487713268826\n",
      "Epoch [316/500], Training Loss: 0.16374557018280028\n",
      "Epoch [316/500], Validation Loss: 0.0352041024182524\n",
      "Epoch [317/500], Training Loss: 0.16393391013145447\n",
      "Epoch [317/500], Validation Loss: 0.035178596419947486\n",
      "Epoch [318/500], Training Loss: 0.1641215682029724\n",
      "Epoch [318/500], Validation Loss: 0.03515109419822693\n",
      "Epoch [319/500], Training Loss: 0.16426462769508363\n",
      "Epoch [319/500], Validation Loss: 0.035087606736591885\n",
      "Epoch [320/500], Training Loss: 0.16446435272693635\n",
      "Epoch [320/500], Validation Loss: 0.035235297467027395\n",
      "Epoch [321/500], Training Loss: 0.16461610317230224\n",
      "Epoch [321/500], Validation Loss: 0.03508170321583748\n",
      "Epoch [322/500], Training Loss: 0.16470821619033812\n",
      "Epoch [322/500], Validation Loss: 0.03499112171786172\n",
      "Epoch [323/500], Training Loss: 0.16487712264060975\n",
      "Epoch [323/500], Validation Loss: 0.035115207944597514\n",
      "Epoch [324/500], Training Loss: 0.165054549574852\n",
      "Epoch [324/500], Validation Loss: 0.03507909870573452\n",
      "Epoch [325/500], Training Loss: 0.1651979047060013\n",
      "Epoch [325/500], Validation Loss: 0.03504272124597004\n",
      "Epoch [326/500], Training Loss: 0.1653859496116638\n",
      "Epoch [326/500], Validation Loss: 0.03503325634769031\n",
      "Epoch [327/500], Training Loss: 0.1655086725950241\n",
      "Epoch [327/500], Validation Loss: 0.03497788895453725\n",
      "Epoch [328/500], Training Loss: 0.16565751135349274\n",
      "Epoch [328/500], Validation Loss: 0.035015475537095754\n",
      "Epoch [329/500], Training Loss: 0.1657915508747101\n",
      "Epoch [329/500], Validation Loss: 0.03505202914987292\n",
      "Epoch [330/500], Training Loss: 0.16597749054431915\n",
      "Epoch [330/500], Validation Loss: 0.03513479286006519\n",
      "Epoch [331/500], Training Loss: 0.1661518269777298\n",
      "Epoch [331/500], Validation Loss: 0.034987744476114004\n",
      "Epoch [332/500], Training Loss: 0.16631972670555115\n",
      "Epoch [332/500], Validation Loss: 0.03503048047423363\n",
      "Epoch [333/500], Training Loss: 0.16651070654392242\n",
      "Epoch [333/500], Validation Loss: 0.0350762451333659\n",
      "Epoch [334/500], Training Loss: 0.16667227685451508\n",
      "Epoch [334/500], Validation Loss: 0.0349939726293087\n",
      "Epoch [335/500], Training Loss: 0.1667676967382431\n",
      "Epoch [335/500], Validation Loss: 0.03509821742773056\n",
      "Epoch [336/500], Training Loss: 0.16688222408294678\n",
      "Epoch [336/500], Validation Loss: 0.03502124920487404\n",
      "Epoch [337/500], Training Loss: 0.16713329434394836\n",
      "Epoch [337/500], Validation Loss: 0.034943493349211555\n",
      "Epoch [338/500], Training Loss: 0.16733642160892487\n",
      "Epoch [338/500], Validation Loss: 0.03493432328104973\n",
      "Epoch [339/500], Training Loss: 0.16747020900249482\n",
      "Epoch [339/500], Validation Loss: 0.03500027475612504\n",
      "Epoch [340/500], Training Loss: 0.1675766885280609\n",
      "Epoch [340/500], Validation Loss: 0.03490960757647242\n",
      "Epoch [341/500], Training Loss: 0.16774398386478423\n",
      "Epoch [341/500], Validation Loss: 0.03509841486811638\n",
      "Epoch [342/500], Training Loss: 0.16796453893184662\n",
      "Epoch [342/500], Validation Loss: 0.03494005863155637\n",
      "Epoch [343/500], Training Loss: 0.16810285151004792\n",
      "Epoch [343/500], Validation Loss: 0.03492021879979542\n",
      "Epoch [344/500], Training Loss: 0.16831252098083496\n",
      "Epoch [344/500], Validation Loss: 0.034989893436431885\n",
      "Epoch [345/500], Training Loss: 0.16842510998249055\n",
      "Epoch [345/500], Validation Loss: 0.03497949295810291\n",
      "Epoch [346/500], Training Loss: 0.16860028743743896\n",
      "Epoch [346/500], Validation Loss: 0.03491953653948648\n",
      "Epoch [347/500], Training Loss: 0.16880710244178773\n",
      "Epoch [347/500], Validation Loss: 0.03496561412300382\n",
      "Epoch [348/500], Training Loss: 0.1690405410528183\n",
      "Epoch [348/500], Validation Loss: 0.03497702734810965\n",
      "Epoch [349/500], Training Loss: 0.16917256534099578\n",
      "Epoch [349/500], Validation Loss: 0.03494072013667652\n",
      "Epoch [350/500], Training Loss: 0.16927738845348358\n",
      "Epoch [350/500], Validation Loss: 0.03495797674570765\n",
      "Epoch [351/500], Training Loss: 0.16943517744541167\n",
      "Epoch [351/500], Validation Loss: 0.03491949928658349\n",
      "Epoch [352/500], Training Loss: 0.1695755821466446\n",
      "Epoch [352/500], Validation Loss: 0.03487248346209526\n",
      "Epoch [353/500], Training Loss: 0.16975724935531616\n",
      "Epoch [353/500], Validation Loss: 0.03486381843686104\n",
      "Epoch [354/500], Training Loss: 0.16988404154777526\n",
      "Epoch [354/500], Validation Loss: 0.03488194410290037\n",
      "Epoch [355/500], Training Loss: 0.17004176020622253\n",
      "Epoch [355/500], Validation Loss: 0.03493387198873928\n",
      "Epoch [356/500], Training Loss: 0.17027327358722688\n",
      "Epoch [356/500], Validation Loss: 0.03489594853350094\n",
      "Epoch [357/500], Training Loss: 0.17039258658885956\n",
      "Epoch [357/500], Validation Loss: 0.034858320972749164\n",
      "Epoch [358/500], Training Loss: 0.17059338986873626\n",
      "Epoch [358/500], Validation Loss: 0.034944252244063785\n",
      "Epoch [359/500], Training Loss: 0.17082456827163697\n",
      "Epoch [359/500], Validation Loss: 0.03489245793649128\n",
      "Epoch [360/500], Training Loss: 0.1709098380804062\n",
      "Epoch [360/500], Validation Loss: 0.03475957417062351\n",
      "Epoch [361/500], Training Loss: 0.17102739751338958\n",
      "Epoch [361/500], Validation Loss: 0.034861545477594645\n",
      "Epoch [362/500], Training Loss: 0.17115490913391113\n",
      "Epoch [362/500], Validation Loss: 0.03487509010093553\n",
      "Epoch [363/500], Training Loss: 0.17135862171649932\n",
      "Epoch [363/500], Validation Loss: 0.0347744556409972\n",
      "Epoch [364/500], Training Loss: 0.1715146344900131\n",
      "Epoch [364/500], Validation Loss: 0.03503656068018505\n",
      "Epoch [365/500], Training Loss: 0.17174625217914583\n",
      "Epoch [365/500], Validation Loss: 0.03496550555740084\n",
      "Epoch [366/500], Training Loss: 0.1719950431585312\n",
      "Epoch [366/500], Validation Loss: 0.03486976293580873\n",
      "Epoch [367/500], Training Loss: 0.17212198674678802\n",
      "Epoch [367/500], Validation Loss: 0.03476821897285325\n",
      "Epoch [368/500], Training Loss: 0.1722872358560562\n",
      "Epoch [368/500], Validation Loss: 0.03486351988145283\n",
      "Epoch [369/500], Training Loss: 0.17242593705654144\n",
      "Epoch [369/500], Validation Loss: 0.034764471330813\n",
      "Epoch [370/500], Training Loss: 0.1725220113992691\n",
      "Epoch [370/500], Validation Loss: 0.034856642463377545\n",
      "Epoch [371/500], Training Loss: 0.17268593192100526\n",
      "Epoch [371/500], Validation Loss: 0.034819754106657844\n",
      "Epoch [372/500], Training Loss: 0.17291776835918427\n",
      "Epoch [372/500], Validation Loss: 0.035001290163823535\n",
      "Epoch [373/500], Training Loss: 0.1732085484266281\n",
      "Epoch [373/500], Validation Loss: 0.03504608252218792\n",
      "Epoch [374/500], Training Loss: 0.17325876116752625\n",
      "Epoch [374/500], Validation Loss: 0.03470944772873606\n",
      "Epoch [375/500], Training Loss: 0.17343719482421874\n",
      "Epoch [375/500], Validation Loss: 0.034743331904922216\n",
      "Epoch [376/500], Training Loss: 0.17352564215660096\n",
      "Epoch [376/500], Validation Loss: 0.03473668598702976\n",
      "Epoch [377/500], Training Loss: 0.1736851805448532\n",
      "Epoch [377/500], Validation Loss: 0.03478877406035151\n",
      "Epoch [378/500], Training Loss: 0.17387003183364869\n",
      "Epoch [378/500], Validation Loss: 0.03480012555207525\n",
      "Epoch [379/500], Training Loss: 0.1739991420507431\n",
      "Epoch [379/500], Validation Loss: 0.03474982987557139\n",
      "Epoch [380/500], Training Loss: 0.1741884779930115\n",
      "Epoch [380/500], Validation Loss: 0.034851424396038055\n",
      "Epoch [381/500], Training Loss: 0.17438199877738952\n",
      "Epoch [381/500], Validation Loss: 0.03479791858366558\n",
      "Epoch [382/500], Training Loss: 0.17449596643447876\n",
      "Epoch [382/500], Validation Loss: 0.03471549653581211\n",
      "Epoch [383/500], Training Loss: 0.17465944468975067\n",
      "Epoch [383/500], Validation Loss: 0.034718581076179235\n",
      "Epoch [384/500], Training Loss: 0.1748286533355713\n",
      "Epoch [384/500], Validation Loss: 0.03479372177805219\n",
      "Epoch [385/500], Training Loss: 0.1750318670272827\n",
      "Epoch [385/500], Validation Loss: 0.0347807024206434\n",
      "Epoch [386/500], Training Loss: 0.1752018290758133\n",
      "Epoch [386/500], Validation Loss: 0.034632932394742966\n",
      "Epoch [387/500], Training Loss: 0.17533710479736328\n",
      "Epoch [387/500], Validation Loss: 0.034664253039019446\n",
      "Epoch [388/500], Training Loss: 0.1755102288722992\n",
      "Epoch [388/500], Validation Loss: 0.03477864872132029\n",
      "Epoch [389/500], Training Loss: 0.17562952220439912\n",
      "Epoch [389/500], Validation Loss: 0.03468458407691547\n",
      "Epoch [390/500], Training Loss: 0.17584899067878723\n",
      "Epoch [390/500], Validation Loss: 0.03476548407758985\n",
      "Epoch [391/500], Training Loss: 0.17597905039787293\n",
      "Epoch [391/500], Validation Loss: 0.03479608307991709\n",
      "Epoch [392/500], Training Loss: 0.1761827450990677\n",
      "Epoch [392/500], Validation Loss: 0.0348415481192725\n",
      "Epoch [393/500], Training Loss: 0.17630332469940185\n",
      "Epoch [393/500], Validation Loss: 0.034620792738028934\n",
      "Epoch [394/500], Training Loss: 0.17649760186672211\n",
      "Epoch [394/500], Validation Loss: 0.034727336040564945\n",
      "Epoch [395/500], Training Loss: 0.17664994776248932\n",
      "Epoch [395/500], Validation Loss: 0.034671025084597726\n",
      "Epoch [396/500], Training Loss: 0.1767773711681366\n",
      "Epoch [396/500], Validation Loss: 0.034881587007216046\n",
      "Epoch [397/500], Training Loss: 0.17710568904876708\n",
      "Epoch [397/500], Validation Loss: 0.03470659362418311\n",
      "Epoch [398/500], Training Loss: 0.17723113059997558\n",
      "Epoch [398/500], Validation Loss: 0.03467408354793276\n",
      "Epoch [399/500], Training Loss: 0.17734971582889558\n",
      "Epoch [399/500], Validation Loss: 0.03469732989157949\n",
      "Epoch [400/500], Training Loss: 0.17755910217761994\n",
      "Epoch [400/500], Validation Loss: 0.0346462396638734\n",
      "Epoch [401/500], Training Loss: 0.17766042709350585\n",
      "Epoch [401/500], Validation Loss: 0.03461758898837226\n",
      "Epoch [402/500], Training Loss: 0.17781964123249053\n",
      "Epoch [402/500], Validation Loss: 0.03464699909090996\n",
      "Epoch [403/500], Training Loss: 0.17797926425933838\n",
      "Epoch [403/500], Validation Loss: 0.034705202494348796\n",
      "Epoch [404/500], Training Loss: 0.17814040422439575\n",
      "Epoch [404/500], Validation Loss: 0.034712904798133035\n",
      "Epoch [405/500], Training Loss: 0.17828397572040558\n",
      "Epoch [405/500], Validation Loss: 0.03477821818419865\n",
      "Epoch [406/500], Training Loss: 0.17840927302837373\n",
      "Epoch [406/500], Validation Loss: 0.03459219581314495\n",
      "Epoch [407/500], Training Loss: 0.17851033210754394\n",
      "Epoch [407/500], Validation Loss: 0.034574003624064584\n",
      "Epoch [408/500], Training Loss: 0.1786661124229431\n",
      "Epoch [408/500], Validation Loss: 0.03464507258364132\n",
      "Epoch [409/500], Training Loss: 0.17880634546279908\n",
      "Epoch [409/500], Validation Loss: 0.034803983356271474\n",
      "Epoch [410/500], Training Loss: 0.17903083443641662\n",
      "Epoch [410/500], Validation Loss: 0.03465599886008671\n",
      "Epoch [411/500], Training Loss: 0.17920762419700623\n",
      "Epoch [411/500], Validation Loss: 0.03463907593062946\n",
      "Epoch [412/500], Training Loss: 0.17934599995613099\n",
      "Epoch [412/500], Validation Loss: 0.03457063330071313\n",
      "Epoch [413/500], Training Loss: 0.1795233553647995\n",
      "Epoch [413/500], Validation Loss: 0.03460279107093811\n",
      "Epoch [414/500], Training Loss: 0.17966636419296264\n",
      "Epoch [414/500], Validation Loss: 0.03464366282735552\n",
      "Epoch [415/500], Training Loss: 0.1799693387746811\n",
      "Epoch [415/500], Validation Loss: 0.03465065945472036\n",
      "Epoch [416/500], Training Loss: 0.18009749829769134\n",
      "Epoch [416/500], Validation Loss: 0.03476893848606518\n",
      "Epoch [417/500], Training Loss: 0.1803513550758362\n",
      "Epoch [417/500], Validation Loss: 0.034603828830378394\n",
      "Epoch [418/500], Training Loss: 0.1804308319091797\n",
      "Epoch [418/500], Validation Loss: 0.034614147884505134\n",
      "Epoch [419/500], Training Loss: 0.18060540914535522\n",
      "Epoch [419/500], Validation Loss: 0.034550536423921585\n",
      "Epoch [420/500], Training Loss: 0.18072695314884185\n",
      "Epoch [420/500], Validation Loss: 0.03456070806298937\n",
      "Epoch [421/500], Training Loss: 0.18088395595550538\n",
      "Epoch [421/500], Validation Loss: 0.034610340637820106\n",
      "Epoch [422/500], Training Loss: 0.1809696739912033\n",
      "Epoch [422/500], Validation Loss: 0.034671135778938024\n",
      "Epoch [423/500], Training Loss: 0.18118769884109498\n",
      "Epoch [423/500], Validation Loss: 0.034608955894197733\n",
      "Epoch [424/500], Training Loss: 0.18129253327846528\n",
      "Epoch [424/500], Validation Loss: 0.03448276913591793\n",
      "Epoch [425/500], Training Loss: 0.1815677696466446\n",
      "Epoch [425/500], Validation Loss: 0.0345030991094453\n",
      "Epoch [426/500], Training Loss: 0.18162464022636413\n",
      "Epoch [426/500], Validation Loss: 0.03454577497073582\n",
      "Epoch [427/500], Training Loss: 0.18175683915615082\n",
      "Epoch [427/500], Validation Loss: 0.034446797732795985\n",
      "Epoch [428/500], Training Loss: 0.18197322726249696\n",
      "Epoch [428/500], Validation Loss: 0.034481429627963474\n",
      "Epoch [429/500], Training Loss: 0.18218278586864473\n",
      "Epoch [429/500], Validation Loss: 0.03451238093631608\n",
      "Epoch [430/500], Training Loss: 0.18232765972614287\n",
      "Epoch [430/500], Validation Loss: 0.03468695974775723\n",
      "Epoch [431/500], Training Loss: 0.18249863207340242\n",
      "Epoch [431/500], Validation Loss: 0.03462122114641326\n",
      "Epoch [432/500], Training Loss: 0.18262435972690583\n",
      "Epoch [432/500], Validation Loss: 0.03473802549498422\n",
      "Epoch [433/500], Training Loss: 0.18279529809951783\n",
      "Epoch [433/500], Validation Loss: 0.034460866557700295\n",
      "Epoch [434/500], Training Loss: 0.18287588953971862\n",
      "Epoch [434/500], Validation Loss: 0.03454615388597761\n",
      "Epoch [435/500], Training Loss: 0.18306654512882234\n",
      "Epoch [435/500], Validation Loss: 0.03466771915555\n",
      "Epoch [436/500], Training Loss: 0.18331595957279206\n",
      "Epoch [436/500], Validation Loss: 0.03448733101998057\n",
      "Epoch [437/500], Training Loss: 0.18339445888996125\n",
      "Epoch [437/500], Validation Loss: 0.03448660245963505\n",
      "Epoch [438/500], Training Loss: 0.18350786626338958\n",
      "Epoch [438/500], Validation Loss: 0.03441671335271427\n",
      "Epoch [439/500], Training Loss: 0.1836035019159317\n",
      "Epoch [439/500], Validation Loss: 0.03436221395220075\n",
      "Epoch [440/500], Training Loss: 0.18378215610980989\n",
      "Epoch [440/500], Validation Loss: 0.03466945620519774\n",
      "Epoch [441/500], Training Loss: 0.18397207498550416\n",
      "Epoch [441/500], Validation Loss: 0.03440135131989207\n",
      "Epoch [442/500], Training Loss: 0.18416276335716247\n",
      "Epoch [442/500], Validation Loss: 0.034489152154752185\n",
      "Epoch [443/500], Training Loss: 0.1842395281791687\n",
      "Epoch [443/500], Validation Loss: 0.034419922956398556\n",
      "Epoch [444/500], Training Loss: 0.18434303462505341\n",
      "Epoch [444/500], Validation Loss: 0.03448172765118735\n",
      "Epoch [445/500], Training Loss: 0.1845128071308136\n",
      "Epoch [445/500], Validation Loss: 0.03439253355775561\n",
      "Epoch [446/500], Training Loss: 0.1846773898601532\n",
      "Epoch [446/500], Validation Loss: 0.03447500988841057\n",
      "Epoch [447/500], Training Loss: 0.18479451060295105\n",
      "Epoch [447/500], Validation Loss: 0.03436369714992387\n",
      "Epoch [448/500], Training Loss: 0.18497855484485626\n",
      "Epoch [448/500], Validation Loss: 0.034526282123156955\n",
      "Epoch [449/500], Training Loss: 0.185129292011261\n",
      "Epoch [449/500], Validation Loss: 0.03444557690194675\n",
      "Epoch [450/500], Training Loss: 0.18521064579486846\n",
      "Epoch [450/500], Validation Loss: 0.03447056881019047\n",
      "Epoch [451/500], Training Loss: 0.18534148633480071\n",
      "Epoch [451/500], Validation Loss: 0.034382768507514684\n",
      "Epoch [452/500], Training Loss: 0.18547671735286714\n",
      "Epoch [452/500], Validation Loss: 0.03447118986930166\n",
      "Epoch [453/500], Training Loss: 0.1856279593706131\n",
      "Epoch [453/500], Validation Loss: 0.0344099865428039\n",
      "Epoch [454/500], Training Loss: 0.1858125591278076\n",
      "Epoch [454/500], Validation Loss: 0.034407543284552436\n",
      "Epoch [455/500], Training Loss: 0.1859176981449127\n",
      "Epoch [455/500], Validation Loss: 0.03438487223216465\n",
      "Epoch [456/500], Training Loss: 0.18606651246547698\n",
      "Epoch [456/500], Validation Loss: 0.0343790166079998\n",
      "Epoch [457/500], Training Loss: 0.186289444565773\n",
      "Epoch [457/500], Validation Loss: 0.034439027309417725\n",
      "Epoch [458/500], Training Loss: 0.1864992618560791\n",
      "Epoch [458/500], Validation Loss: 0.034364666257585795\n",
      "Epoch [459/500], Training Loss: 0.18662016808986664\n",
      "Epoch [459/500], Validation Loss: 0.03441148943134716\n",
      "Epoch [460/500], Training Loss: 0.18675754308700562\n",
      "Epoch [460/500], Validation Loss: 0.03442088833877018\n",
      "Epoch [461/500], Training Loss: 0.18693628311157226\n",
      "Epoch [461/500], Validation Loss: 0.03447224625519344\n",
      "Epoch [462/500], Training Loss: 0.18705970764160157\n",
      "Epoch [462/500], Validation Loss: 0.03435220356498446\n",
      "Epoch [463/500], Training Loss: 0.1872539609670639\n",
      "Epoch [463/500], Validation Loss: 0.034361341169902256\n",
      "Epoch [464/500], Training Loss: 0.18733038485050202\n",
      "Epoch [464/500], Validation Loss: 0.034380661589758735\n",
      "Epoch [465/500], Training Loss: 0.1875027137994766\n",
      "Epoch [465/500], Validation Loss: 0.03444153442978859\n",
      "Epoch [466/500], Training Loss: 0.18765514135360717\n",
      "Epoch [466/500], Validation Loss: 0.03450308367609978\n",
      "Epoch [467/500], Training Loss: 0.18778855919837953\n",
      "Epoch [467/500], Validation Loss: 0.03463062910096986\n",
      "Epoch [468/500], Training Loss: 0.1879442024230957\n",
      "Epoch [468/500], Validation Loss: 0.03442248861704554\n",
      "Epoch [469/500], Training Loss: 0.18801558375358582\n",
      "Epoch [469/500], Validation Loss: 0.03434924674885614\n",
      "Epoch [470/500], Training Loss: 0.18817155063152313\n",
      "Epoch [470/500], Validation Loss: 0.034360986202955246\n",
      "Epoch [471/500], Training Loss: 0.18830521941184997\n",
      "Epoch [471/500], Validation Loss: 0.03433406086904662\n",
      "Epoch [472/500], Training Loss: 0.18849010050296783\n",
      "Epoch [472/500], Validation Loss: 0.034562841589961736\n",
      "Epoch [473/500], Training Loss: 0.1886461544036865\n",
      "Epoch [473/500], Validation Loss: 0.034435807062046866\n",
      "Epoch [474/500], Training Loss: 0.1887791907787323\n",
      "Epoch [474/500], Validation Loss: 0.034397821341242106\n",
      "Epoch [475/500], Training Loss: 0.18897260010242461\n",
      "Epoch [475/500], Validation Loss: 0.03441814546074186\n",
      "Epoch [476/500], Training Loss: 0.1891590040922165\n",
      "Epoch [476/500], Validation Loss: 0.034383964857884815\n",
      "Epoch [477/500], Training Loss: 0.18920642375946045\n",
      "Epoch [477/500], Validation Loss: 0.034435956605843136\n",
      "Epoch [478/500], Training Loss: 0.18933521926403046\n",
      "Epoch [478/500], Validation Loss: 0.0344783573278359\n",
      "Epoch [479/500], Training Loss: 0.1894645619392395\n",
      "Epoch [479/500], Validation Loss: 0.03443079814314842\n",
      "Epoch [480/500], Training Loss: 0.18962933301925658\n",
      "Epoch [480/500], Validation Loss: 0.03427102203880038\n",
      "Epoch [481/500], Training Loss: 0.18982230484485627\n",
      "Epoch [481/500], Validation Loss: 0.03435690541352544\n",
      "Epoch [482/500], Training Loss: 0.18996481657028197\n",
      "Epoch [482/500], Validation Loss: 0.03446321402277265\n",
      "Epoch [483/500], Training Loss: 0.1901182985305786\n",
      "Epoch [483/500], Validation Loss: 0.03438252955675125\n",
      "Epoch [484/500], Training Loss: 0.1902855622768402\n",
      "Epoch [484/500], Validation Loss: 0.03452978017074721\n",
      "Epoch [485/500], Training Loss: 0.19045558333396911\n",
      "Epoch [485/500], Validation Loss: 0.034320863229887824\n",
      "Epoch [486/500], Training Loss: 0.19064512431621553\n",
      "Epoch [486/500], Validation Loss: 0.034435736281531196\n",
      "Epoch [487/500], Training Loss: 0.19076850950717927\n",
      "Epoch [487/500], Validation Loss: 0.034399029931851795\n",
      "Epoch [488/500], Training Loss: 0.19091567993164063\n",
      "Epoch [488/500], Validation Loss: 0.03430848675114768\n",
      "Epoch [489/500], Training Loss: 0.19101725816726683\n",
      "Epoch [489/500], Validation Loss: 0.03432813925402505\n",
      "Epoch [490/500], Training Loss: 0.1911699044704437\n",
      "Epoch [490/500], Validation Loss: 0.03452504745551518\n",
      "Epoch [491/500], Training Loss: 0.191355357170105\n",
      "Epoch [491/500], Validation Loss: 0.034270180123192925\n",
      "Epoch [492/500], Training Loss: 0.1914885860681534\n",
      "Epoch [492/500], Validation Loss: 0.03444488399795124\n",
      "Epoch [493/500], Training Loss: 0.19171134173870086\n",
      "Epoch [493/500], Validation Loss: 0.03425091558269092\n",
      "Epoch [494/500], Training Loss: 0.19186993241310119\n",
      "Epoch [494/500], Validation Loss: 0.03453476460916655\n",
      "Epoch [495/500], Training Loss: 0.19209171295166017\n",
      "Epoch [495/500], Validation Loss: 0.034278507211378643\n",
      "Epoch [496/500], Training Loss: 0.1922764015197754\n",
      "Epoch [496/500], Validation Loss: 0.03420589704598699\n",
      "Epoch [497/500], Training Loss: 0.19233995139598847\n",
      "Epoch [497/500], Validation Loss: 0.03436160247240748\n",
      "Epoch [498/500], Training Loss: 0.1925324958562851\n",
      "Epoch [498/500], Validation Loss: 0.03439336482967649\n",
      "Epoch [499/500], Training Loss: 0.19274430155754088\n",
      "Epoch [499/500], Validation Loss: 0.03426485668335642\n",
      "Epoch [500/500], Training Loss: 0.19292131423950196\n",
      "Epoch [500/500], Validation Loss: 0.034221810953957696\n",
      "Epoch [1/200], Training Loss: 0.19697000414133073\n",
      "Epoch [1/200], Validation Loss: 0.05208994182092803\n",
      "Epoch [2/200], Training Loss: 0.11172509133815765\n",
      "Epoch [2/200], Validation Loss: 0.05134443885513714\n",
      "Epoch [3/200], Training Loss: 0.11143078863620758\n",
      "Epoch [3/200], Validation Loss: 0.05075558860387121\n",
      "Epoch [4/200], Training Loss: 0.11082221269607544\n",
      "Epoch [4/200], Validation Loss: 0.05007091271025794\n",
      "Epoch [5/200], Training Loss: 0.11032783180475235\n",
      "Epoch [5/200], Validation Loss: 0.04967958639775004\n",
      "Epoch [6/200], Training Loss: 0.11004347801208496\n",
      "Epoch [6/200], Validation Loss: 0.049436134951455254\n",
      "Epoch [7/200], Training Loss: 0.1098766964673996\n",
      "Epoch [7/200], Validation Loss: 0.049292909247534614\n",
      "Epoch [8/200], Training Loss: 0.10977190911769867\n",
      "Epoch [8/200], Validation Loss: 0.04910224037511008\n",
      "Epoch [9/200], Training Loss: 0.1096902647614479\n",
      "Epoch [9/200], Validation Loss: 0.04910342075995037\n",
      "Epoch [10/200], Training Loss: 0.10948767244815827\n",
      "Epoch [10/200], Validation Loss: 0.04863809847405979\n",
      "Epoch [11/200], Training Loss: 0.10937797486782073\n",
      "Epoch [11/200], Validation Loss: 0.048585506954363415\n",
      "Epoch [12/200], Training Loss: 0.10918813079595566\n",
      "Epoch [12/200], Validation Loss: 0.04808185888188226\n",
      "Epoch [13/200], Training Loss: 0.10898003935813903\n",
      "Epoch [13/200], Validation Loss: 0.047804221510887146\n",
      "Epoch [14/200], Training Loss: 0.10883861660957336\n",
      "Epoch [14/200], Validation Loss: 0.0475465932062694\n",
      "Epoch [15/200], Training Loss: 0.10868985772132873\n",
      "Epoch [15/200], Validation Loss: 0.047349399753979275\n",
      "Epoch [16/200], Training Loss: 0.10859452307224274\n",
      "Epoch [16/200], Validation Loss: 0.04706095691238131\n",
      "Epoch [17/200], Training Loss: 0.10844795882701874\n",
      "Epoch [17/200], Validation Loss: 0.0468386709690094\n",
      "Epoch [18/200], Training Loss: 0.10827305883169175\n",
      "Epoch [18/200], Validation Loss: 0.046599529683589935\n",
      "Epoch [19/200], Training Loss: 0.1080932480096817\n",
      "Epoch [19/200], Validation Loss: 0.04633629641362599\n",
      "Epoch [20/200], Training Loss: 0.10797610282897949\n",
      "Epoch [20/200], Validation Loss: 0.0461563483944961\n",
      "Epoch [21/200], Training Loss: 0.10786420106887817\n",
      "Epoch [21/200], Validation Loss: 0.04588083390678678\n",
      "Epoch [22/200], Training Loss: 0.10774830311536789\n",
      "Epoch [22/200], Validation Loss: 0.045706367918423245\n",
      "Epoch [23/200], Training Loss: 0.10766459047794343\n",
      "Epoch [23/200], Validation Loss: 0.045437094888516834\n",
      "Epoch [24/200], Training Loss: 0.10762851119041443\n",
      "Epoch [24/200], Validation Loss: 0.04540526228291648\n",
      "Epoch [25/200], Training Loss: 0.1075830927491188\n",
      "Epoch [25/200], Validation Loss: 0.04501085728406906\n",
      "Epoch [26/200], Training Loss: 0.1074389323592186\n",
      "Epoch [26/200], Validation Loss: 0.04480640270880291\n",
      "Epoch [27/200], Training Loss: 0.10733121871948242\n",
      "Epoch [27/200], Validation Loss: 0.04471392184495926\n",
      "Epoch [28/200], Training Loss: 0.1072785770893097\n",
      "Epoch [28/200], Validation Loss: 0.044302557195935933\n",
      "Epoch [29/200], Training Loss: 0.10714984804391861\n",
      "Epoch [29/200], Validation Loss: 0.04405790886708668\n",
      "Epoch [30/200], Training Loss: 0.10708417117595673\n",
      "Epoch [30/200], Validation Loss: 0.043899111449718475\n",
      "Epoch [31/200], Training Loss: 0.10700831025838851\n",
      "Epoch [31/200], Validation Loss: 0.04361423956496375\n",
      "Epoch [32/200], Training Loss: 0.10696984052658082\n",
      "Epoch [32/200], Validation Loss: 0.04341268113681248\n",
      "Epoch [33/200], Training Loss: 0.10690729707479477\n",
      "Epoch [33/200], Validation Loss: 0.04317443019577435\n",
      "Epoch [34/200], Training Loss: 0.1068074345588684\n",
      "Epoch [34/200], Validation Loss: 0.043082340487412045\n",
      "Epoch [35/200], Training Loss: 0.10676115870475769\n",
      "Epoch [35/200], Validation Loss: 0.042774900261844905\n",
      "Epoch [36/200], Training Loss: 0.10669975608587265\n",
      "Epoch [36/200], Validation Loss: 0.04260412071432386\n",
      "Epoch [37/200], Training Loss: 0.10658375263214111\n",
      "Epoch [37/200], Validation Loss: 0.04255371753658567\n",
      "Epoch [38/200], Training Loss: 0.10662174701690674\n",
      "Epoch [38/200], Validation Loss: 0.04224801116756031\n",
      "Epoch [39/200], Training Loss: 0.10649077206850052\n",
      "Epoch [39/200], Validation Loss: 0.04204179240124566\n",
      "Epoch [40/200], Training Loss: 0.10647875487804413\n",
      "Epoch [40/200], Validation Loss: 0.04190782510808536\n",
      "Epoch [41/200], Training Loss: 0.1063681611418724\n",
      "Epoch [41/200], Validation Loss: 0.041676772492272515\n",
      "Epoch [42/200], Training Loss: 0.10630565941333771\n",
      "Epoch [42/200], Validation Loss: 0.04151994202818189\n",
      "Epoch [43/200], Training Loss: 0.10634728044271469\n",
      "Epoch [43/200], Validation Loss: 0.04145807400345802\n",
      "Epoch [44/200], Training Loss: 0.10631970167160035\n",
      "Epoch [44/200], Validation Loss: 0.04117250921470778\n",
      "Epoch [45/200], Training Loss: 0.10623175114393234\n",
      "Epoch [45/200], Validation Loss: 0.0410499343914645\n",
      "Epoch [46/200], Training Loss: 0.1061719986796379\n",
      "Epoch [46/200], Validation Loss: 0.0408392750791141\n",
      "Epoch [47/200], Training Loss: 0.10616451114416123\n",
      "Epoch [47/200], Validation Loss: 0.0407338733119624\n",
      "Epoch [48/200], Training Loss: 0.10622038245201111\n",
      "Epoch [48/200], Validation Loss: 0.040790090071303506\n",
      "Epoch [49/200], Training Loss: 0.10623216152191162\n",
      "Epoch [49/200], Validation Loss: 0.04050383876476969\n",
      "Epoch [50/200], Training Loss: 0.1061477267742157\n",
      "Epoch [50/200], Validation Loss: 0.040403007928814204\n",
      "Epoch [51/200], Training Loss: 0.10605318397283554\n",
      "Epoch [51/200], Validation Loss: 0.040296289537634165\n",
      "Epoch [52/200], Training Loss: 0.10644236385822296\n",
      "Epoch [52/200], Validation Loss: 0.04027530071990831\n",
      "Epoch [53/200], Training Loss: 0.10611940383911132\n",
      "Epoch [53/200], Validation Loss: 0.039981244398014884\n",
      "Epoch [54/200], Training Loss: 0.1060184308886528\n",
      "Epoch [54/200], Validation Loss: 0.03994790038892201\n",
      "Epoch [55/200], Training Loss: 0.10597560614347458\n",
      "Epoch [55/200], Validation Loss: 0.03965931013226509\n",
      "Epoch [56/200], Training Loss: 0.10599505186080932\n",
      "Epoch [56/200], Validation Loss: 0.03956912351506097\n",
      "Epoch [57/200], Training Loss: 0.10603480607271194\n",
      "Epoch [57/200], Validation Loss: 0.039512402777160914\n",
      "Epoch [58/200], Training Loss: 0.10603323459625244\n",
      "Epoch [58/200], Validation Loss: 0.0394225540970053\n",
      "Epoch [59/200], Training Loss: 0.10610353946685791\n",
      "Epoch [59/200], Validation Loss: 0.03958645037242344\n",
      "Epoch [60/200], Training Loss: 0.10615423142910003\n",
      "Epoch [60/200], Validation Loss: 0.03925493572439466\n",
      "Epoch [61/200], Training Loss: 0.10596974670886994\n",
      "Epoch [61/200], Validation Loss: 0.039055838116577694\n",
      "Epoch [62/200], Training Loss: 0.10597696572542191\n",
      "Epoch [62/200], Validation Loss: 0.03906403322304998\n",
      "Epoch [63/200], Training Loss: 0.10601469576358795\n",
      "Epoch [63/200], Validation Loss: 0.03887692413159779\n",
      "Epoch [64/200], Training Loss: 0.10606583923101426\n",
      "Epoch [64/200], Validation Loss: 0.03893145280224936\n",
      "Epoch [65/200], Training Loss: 0.1060483154654503\n",
      "Epoch [65/200], Validation Loss: 0.038712279072829654\n",
      "Epoch [66/200], Training Loss: 0.10610732644796371\n",
      "Epoch [66/200], Validation Loss: 0.03873302308576448\n",
      "Epoch [67/200], Training Loss: 0.10602508008480072\n",
      "Epoch [67/200], Validation Loss: 0.038510896265506744\n",
      "Epoch [68/200], Training Loss: 0.10600565850734711\n",
      "Epoch [68/200], Validation Loss: 0.03856380124177251\n",
      "Epoch [69/200], Training Loss: 0.10603023678064347\n",
      "Epoch [69/200], Validation Loss: 0.03893431169646127\n",
      "Epoch [70/200], Training Loss: 0.10613459676504135\n",
      "Epoch [70/200], Validation Loss: 0.03825942373701504\n",
      "Epoch [71/200], Training Loss: 0.10612146586179733\n",
      "Epoch [71/200], Validation Loss: 0.03817669940846307\n",
      "Epoch [72/200], Training Loss: 0.10607028543949128\n",
      "Epoch [72/200], Validation Loss: 0.03812686460358756\n",
      "Epoch [73/200], Training Loss: 0.1060350027680397\n",
      "Epoch [73/200], Validation Loss: 0.03802507211055074\n",
      "Epoch [74/200], Training Loss: 0.10604202389717102\n",
      "Epoch [74/200], Validation Loss: 0.037948786680187495\n",
      "Epoch [75/200], Training Loss: 0.10610883563756943\n",
      "Epoch [75/200], Validation Loss: 0.03787229263356754\n",
      "Epoch [76/200], Training Loss: 0.10613819867372513\n",
      "Epoch [76/200], Validation Loss: 0.037802278995513916\n",
      "Epoch [77/200], Training Loss: 0.10612160205841065\n",
      "Epoch [77/200], Validation Loss: 0.037755162588187625\n",
      "Epoch [78/200], Training Loss: 0.10614472210407257\n",
      "Epoch [78/200], Validation Loss: 0.038008730326380046\n",
      "Epoch [79/200], Training Loss: 0.10624557346105576\n",
      "Epoch [79/200], Validation Loss: 0.03762741067579815\n",
      "Epoch [80/200], Training Loss: 0.10618588387966156\n",
      "Epoch [80/200], Validation Loss: 0.037579604025397985\n",
      "Epoch [81/200], Training Loss: 0.10619328290224075\n",
      "Epoch [81/200], Validation Loss: 0.03749204959188189\n",
      "Epoch [82/200], Training Loss: 0.10623049110174179\n",
      "Epoch [82/200], Validation Loss: 0.0375227832368442\n",
      "Epoch [83/200], Training Loss: 0.10625324428081512\n",
      "Epoch [83/200], Validation Loss: 0.037409301315035136\n",
      "Epoch [84/200], Training Loss: 0.10625225186347961\n",
      "Epoch [84/200], Validation Loss: 0.03733898060662406\n",
      "Epoch [85/200], Training Loss: 0.10632594347000122\n",
      "Epoch [85/200], Validation Loss: 0.037326392850705555\n",
      "Epoch [86/200], Training Loss: 0.1063778731226921\n",
      "Epoch [86/200], Validation Loss: 0.0372807468686785\n",
      "Epoch [87/200], Training Loss: 0.10635810554027557\n",
      "Epoch [87/200], Validation Loss: 0.03715926515204566\n",
      "Epoch [88/200], Training Loss: 0.1064486813545227\n",
      "Epoch [88/200], Validation Loss: 0.03717326798609325\n",
      "Epoch [89/200], Training Loss: 0.10645358115434647\n",
      "Epoch [89/200], Validation Loss: 0.03727846486227853\n",
      "Epoch [90/200], Training Loss: 0.1065377014875412\n",
      "Epoch [90/200], Validation Loss: 0.03705483196037156\n",
      "Epoch [91/200], Training Loss: 0.10653790831565857\n",
      "Epoch [91/200], Validation Loss: 0.037009596824645996\n",
      "Epoch [92/200], Training Loss: 0.10652605891227722\n",
      "Epoch [92/200], Validation Loss: 0.03706405737570354\n",
      "Epoch [93/200], Training Loss: 0.10653023183345794\n",
      "Epoch [93/200], Validation Loss: 0.036985536238976886\n",
      "Epoch [94/200], Training Loss: 0.10657031655311584\n",
      "Epoch [94/200], Validation Loss: 0.036876775324344635\n",
      "Epoch [95/200], Training Loss: 0.10662839591503143\n",
      "Epoch [95/200], Validation Loss: 0.036794385207550864\n",
      "Epoch [96/200], Training Loss: 0.1066392132639885\n",
      "Epoch [96/200], Validation Loss: 0.0368774730179991\n",
      "Epoch [97/200], Training Loss: 0.1066857984662056\n",
      "Epoch [97/200], Validation Loss: 0.036750724273068566\n",
      "Epoch [98/200], Training Loss: 0.10672445058822631\n",
      "Epoch [98/200], Validation Loss: 0.0367718269782407\n",
      "Epoch [99/200], Training Loss: 0.10672111928462982\n",
      "Epoch [99/200], Validation Loss: 0.036664589175156186\n",
      "Epoch [100/200], Training Loss: 0.10677656501531602\n",
      "Epoch [100/200], Validation Loss: 0.03668835652726037\n",
      "Epoch [101/200], Training Loss: 0.10677351236343384\n",
      "Epoch [101/200], Validation Loss: 0.036609823150294166\n",
      "Epoch [102/200], Training Loss: 0.10683514833450318\n",
      "Epoch [102/200], Validation Loss: 0.036521121327366145\n",
      "Epoch [103/200], Training Loss: 0.10685663521289826\n",
      "Epoch [103/200], Validation Loss: 0.03661288054926055\n",
      "Epoch [104/200], Training Loss: 0.1069322469830513\n",
      "Epoch [104/200], Validation Loss: 0.03657505341938564\n",
      "Epoch [105/200], Training Loss: 0.10690956860780716\n",
      "Epoch [105/200], Validation Loss: 0.03644215368798801\n",
      "Epoch [106/200], Training Loss: 0.10699477106332779\n",
      "Epoch [106/200], Validation Loss: 0.0366123973258904\n",
      "Epoch [107/200], Training Loss: 0.10706766217947006\n",
      "Epoch [107/200], Validation Loss: 0.03638786556465285\n",
      "Epoch [108/200], Training Loss: 0.10705293267965317\n",
      "Epoch [108/200], Validation Loss: 0.036393235836710246\n",
      "Epoch [109/200], Training Loss: 0.10705614149570465\n",
      "Epoch [109/200], Validation Loss: 0.03637151579771723\n",
      "Epoch [110/200], Training Loss: 0.10710337668657303\n",
      "Epoch [110/200], Validation Loss: 0.036456002188580375\n",
      "Epoch [111/200], Training Loss: 0.10717591434717179\n",
      "Epoch [111/200], Validation Loss: 0.03631196330700602\n",
      "Epoch [112/200], Training Loss: 0.10712548375129699\n",
      "Epoch [112/200], Validation Loss: 0.03624863337193217\n",
      "Epoch [113/200], Training Loss: 0.10717653393745423\n",
      "Epoch [113/200], Validation Loss: 0.036215132900646756\n",
      "Epoch [114/200], Training Loss: 0.1072190135717392\n",
      "Epoch [114/200], Validation Loss: 0.036139161991221566\n",
      "Epoch [115/200], Training Loss: 0.10724259346723557\n",
      "Epoch [115/200], Validation Loss: 0.03617845954639571\n",
      "Epoch [116/200], Training Loss: 0.10730173498392105\n",
      "Epoch [116/200], Validation Loss: 0.036305000207253864\n",
      "Epoch [117/200], Training Loss: 0.10736873865127564\n",
      "Epoch [117/200], Validation Loss: 0.03616643164839063\n",
      "Epoch [118/200], Training Loss: 0.10735653221607208\n",
      "Epoch [118/200], Validation Loss: 0.03605564311146736\n",
      "Epoch [119/200], Training Loss: 0.10740917474031449\n",
      "Epoch [119/200], Validation Loss: 0.03605419716664723\n",
      "Epoch [120/200], Training Loss: 0.10742275059223175\n",
      "Epoch [120/200], Validation Loss: 0.03606879498277392\n",
      "Epoch [121/200], Training Loss: 0.10755686670541763\n",
      "Epoch [121/200], Validation Loss: 0.03602266524519239\n",
      "Epoch [122/200], Training Loss: 0.10750541657209396\n",
      "Epoch [122/200], Validation Loss: 0.03596566936799458\n",
      "Epoch [123/200], Training Loss: 0.10754633367061615\n",
      "Epoch [123/200], Validation Loss: 0.035992804914712906\n",
      "Epoch [124/200], Training Loss: 0.10759529113769531\n",
      "Epoch [124/200], Validation Loss: 0.035989365939583094\n",
      "Epoch [125/200], Training Loss: 0.10768184036016465\n",
      "Epoch [125/200], Validation Loss: 0.03595968122993197\n",
      "Epoch [126/200], Training Loss: 0.10771921992301942\n",
      "Epoch [126/200], Validation Loss: 0.03588775971106121\n",
      "Epoch [127/200], Training Loss: 0.10769759386777877\n",
      "Epoch [127/200], Validation Loss: 0.03596148054514613\n",
      "Epoch [128/200], Training Loss: 0.10780657291412353\n",
      "Epoch [128/200], Validation Loss: 0.03596093878149986\n",
      "Epoch [129/200], Training Loss: 0.10779071152210236\n",
      "Epoch [129/200], Validation Loss: 0.035854043172938485\n",
      "Epoch [130/200], Training Loss: 0.10783366113901138\n",
      "Epoch [130/200], Validation Loss: 0.03583576051252229\n",
      "Epoch [131/200], Training Loss: 0.10788883328437805\n",
      "Epoch [131/200], Validation Loss: 0.03586991024868829\n",
      "Epoch [132/200], Training Loss: 0.10789514005184174\n",
      "Epoch [132/200], Validation Loss: 0.0358257240482739\n",
      "Epoch [133/200], Training Loss: 0.1080395931005478\n",
      "Epoch [133/200], Validation Loss: 0.0357767998107842\n",
      "Epoch [134/200], Training Loss: 0.1080124792456627\n",
      "Epoch [134/200], Validation Loss: 0.036023594971214025\n",
      "Epoch [135/200], Training Loss: 0.10804040908813477\n",
      "Epoch [135/200], Validation Loss: 0.03581657526748521\n",
      "Epoch [136/200], Training Loss: 0.10805458217859268\n",
      "Epoch [136/200], Validation Loss: 0.03575048329574721\n",
      "Epoch [137/200], Training Loss: 0.10808906555175782\n",
      "Epoch [137/200], Validation Loss: 0.035658259476934163\n",
      "Epoch [138/200], Training Loss: 0.10811853110790252\n",
      "Epoch [138/200], Validation Loss: 0.03565720734851701\n",
      "Epoch [139/200], Training Loss: 0.1082277113199234\n",
      "Epoch [139/200], Validation Loss: 0.035662775593144555\n",
      "Epoch [140/200], Training Loss: 0.10835836946964264\n",
      "Epoch [140/200], Validation Loss: 0.03589545988610813\n",
      "Epoch [141/200], Training Loss: 0.1083046880364418\n",
      "Epoch [141/200], Validation Loss: 0.03563454800418445\n",
      "Epoch [142/200], Training Loss: 0.10827201128005981\n",
      "Epoch [142/200], Validation Loss: 0.0355954643871103\n",
      "Epoch [143/200], Training Loss: 0.10834082961082458\n",
      "Epoch [143/200], Validation Loss: 0.035605388560465405\n",
      "Epoch [144/200], Training Loss: 0.10835924059152603\n",
      "Epoch [144/200], Validation Loss: 0.035558784646647315\n",
      "Epoch [145/200], Training Loss: 0.10840833604335785\n",
      "Epoch [145/200], Validation Loss: 0.03557012868779046\n",
      "Epoch [146/200], Training Loss: 0.10842794805765152\n",
      "Epoch [146/200], Validation Loss: 0.035696598035948615\n",
      "Epoch [147/200], Training Loss: 0.10857396721839904\n",
      "Epoch [147/200], Validation Loss: 0.03561640477606228\n",
      "Epoch [148/200], Training Loss: 0.1086189904808998\n",
      "Epoch [148/200], Validation Loss: 0.035532682069710324\n",
      "Epoch [149/200], Training Loss: 0.10853441625833511\n",
      "Epoch [149/200], Validation Loss: 0.03549318867070334\n",
      "Epoch [150/200], Training Loss: 0.10859420955181122\n",
      "Epoch [150/200], Validation Loss: 0.03546822869351932\n",
      "Epoch [151/200], Training Loss: 0.10863535970449448\n",
      "Epoch [151/200], Validation Loss: 0.03542765176721981\n",
      "Epoch [152/200], Training Loss: 0.10867491006851196\n",
      "Epoch [152/200], Validation Loss: 0.03543059527873993\n",
      "Epoch [153/200], Training Loss: 0.10869456112384795\n",
      "Epoch [153/200], Validation Loss: 0.03545574737446649\n",
      "Epoch [154/200], Training Loss: 0.10880242049694061\n",
      "Epoch [154/200], Validation Loss: 0.035474052386624474\n",
      "Epoch [155/200], Training Loss: 0.10881367534399032\n",
      "Epoch [155/200], Validation Loss: 0.035394834620612006\n",
      "Epoch [156/200], Training Loss: 0.10886440813541412\n",
      "Epoch [156/200], Validation Loss: 0.035415618015187125\n",
      "Epoch [157/200], Training Loss: 0.10893148601055146\n",
      "Epoch [157/200], Validation Loss: 0.03546615157808576\n",
      "Epoch [158/200], Training Loss: 0.10900056272745133\n",
      "Epoch [158/200], Validation Loss: 0.03556716016360691\n",
      "Epoch [159/200], Training Loss: 0.10909348011016845\n",
      "Epoch [159/200], Validation Loss: 0.03539842686482838\n",
      "Epoch [160/200], Training Loss: 0.10909689068794251\n",
      "Epoch [160/200], Validation Loss: 0.035337193736008236\n",
      "Epoch [161/200], Training Loss: 0.10907641232013703\n",
      "Epoch [161/200], Validation Loss: 0.035334863300834386\n",
      "Epoch [162/200], Training Loss: 0.10913315802812576\n",
      "Epoch [162/200], Validation Loss: 0.03539374417492321\n",
      "Epoch [163/200], Training Loss: 0.10915296167135238\n",
      "Epoch [163/200], Validation Loss: 0.03531318370785032\n",
      "Epoch [164/200], Training Loss: 0.10927754491567612\n",
      "Epoch [164/200], Validation Loss: 0.035396243312529156\n",
      "Epoch [165/200], Training Loss: 0.10933222860097885\n",
      "Epoch [165/200], Validation Loss: 0.03535238440547671\n",
      "Epoch [166/200], Training Loss: 0.10944202959537506\n",
      "Epoch [166/200], Validation Loss: 0.03531411715916225\n",
      "Epoch [167/200], Training Loss: 0.10934386491775512\n",
      "Epoch [167/200], Validation Loss: 0.035293184752975194\n",
      "Epoch [168/200], Training Loss: 0.10942442625761033\n",
      "Epoch [168/200], Validation Loss: 0.03526907839945385\n",
      "Epoch [169/200], Training Loss: 0.10943784534931184\n",
      "Epoch [169/200], Validation Loss: 0.035226303551878245\n",
      "Epoch [170/200], Training Loss: 0.10945861130952834\n",
      "Epoch [170/200], Validation Loss: 0.035200235566922596\n",
      "Epoch [171/200], Training Loss: 0.10951488882303238\n",
      "Epoch [171/200], Validation Loss: 0.03521351516246796\n",
      "Epoch [172/200], Training Loss: 0.10953005582094193\n",
      "Epoch [172/200], Validation Loss: 0.03528429301721709\n",
      "Epoch [173/200], Training Loss: 0.1095825582742691\n",
      "Epoch [173/200], Validation Loss: 0.035186122570719035\n",
      "Epoch [174/200], Training Loss: 0.10973145723342896\n",
      "Epoch [174/200], Validation Loss: 0.035247367939778736\n",
      "Epoch [175/200], Training Loss: 0.10970303922891617\n",
      "Epoch [175/200], Validation Loss: 0.03523727080651692\n",
      "Epoch [176/200], Training Loss: 0.10972137987613678\n",
      "Epoch [176/200], Validation Loss: 0.03518431101526533\n",
      "Epoch [177/200], Training Loss: 0.10973258703947067\n",
      "Epoch [177/200], Validation Loss: 0.03515222828303065\n",
      "Epoch [178/200], Training Loss: 0.10980543941259384\n",
      "Epoch [178/200], Validation Loss: 0.035186883594308584\n",
      "Epoch [179/200], Training Loss: 0.10980437427759171\n",
      "Epoch [179/200], Validation Loss: 0.03513442991035325\n",
      "Epoch [180/200], Training Loss: 0.1098651447892189\n",
      "Epoch [180/200], Validation Loss: 0.0351134075650147\n",
      "Epoch [181/200], Training Loss: 0.10991856902837753\n",
      "Epoch [181/200], Validation Loss: 0.03511566136564527\n",
      "Epoch [182/200], Training Loss: 0.10997127145528793\n",
      "Epoch [182/200], Validation Loss: 0.03523762843438557\n",
      "Epoch [183/200], Training Loss: 0.11008239090442658\n",
      "Epoch [183/200], Validation Loss: 0.0352535934320518\n",
      "Epoch [184/200], Training Loss: 0.11009848833084107\n",
      "Epoch [184/200], Validation Loss: 0.03507793215768678\n",
      "Epoch [185/200], Training Loss: 0.11010380208492279\n",
      "Epoch [185/200], Validation Loss: 0.03517972358635494\n",
      "Epoch [186/200], Training Loss: 0.11015445232391358\n",
      "Epoch [186/200], Validation Loss: 0.03506225560392652\n",
      "Epoch [187/200], Training Loss: 0.11017499327659606\n",
      "Epoch [187/200], Validation Loss: 0.035084787224020274\n",
      "Epoch [188/200], Training Loss: 0.1102725350856781\n",
      "Epoch [188/200], Validation Loss: 0.03504407512290137\n",
      "Epoch [189/200], Training Loss: 0.1102990299463272\n",
      "Epoch [189/200], Validation Loss: 0.035035068435328345\n",
      "Epoch [190/200], Training Loss: 0.11034431725740433\n",
      "Epoch [190/200], Validation Loss: 0.03504546784928867\n",
      "Epoch [191/200], Training Loss: 0.11042357653379441\n",
      "Epoch [191/200], Validation Loss: 0.035099255719355175\n",
      "Epoch [192/200], Training Loss: 0.11043935805559159\n",
      "Epoch [192/200], Validation Loss: 0.03503608011773655\n",
      "Epoch [193/200], Training Loss: 0.1104816722869873\n",
      "Epoch [193/200], Validation Loss: 0.03502130721296583\n",
      "Epoch [194/200], Training Loss: 0.11054184257984162\n",
      "Epoch [194/200], Validation Loss: 0.0350563611303057\n",
      "Epoch [195/200], Training Loss: 0.11059688031673431\n",
      "Epoch [195/200], Validation Loss: 0.0350237637758255\n",
      "Epoch [196/200], Training Loss: 0.1105962136387825\n",
      "Epoch [196/200], Validation Loss: 0.03500494360923767\n",
      "Epoch [197/200], Training Loss: 0.11064612776041031\n",
      "Epoch [197/200], Validation Loss: 0.03498906535761697\n",
      "Epoch [198/200], Training Loss: 0.11070876568555832\n",
      "Epoch [198/200], Validation Loss: 0.03497281351259777\n",
      "Epoch [199/200], Training Loss: 0.11072908312082291\n",
      "Epoch [199/200], Validation Loss: 0.03502264672092029\n",
      "Epoch [200/200], Training Loss: 0.11079230666160583\n",
      "Epoch [200/200], Validation Loss: 0.034995388239622116\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DiffusionModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs = data[0].to(device)\n",
    "        targets = data[0].to(device) # 使用输入数据作为目标数据，WEIL重构输入\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        l2_reg = calc_l2_reg(model)\n",
    "        total_loss = loss + reg_coeff * l2_reg\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            inputs = data[0].to(device)\n",
    "            targets = data[0].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss/len(val_loader)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0356166479177773\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        inputs = data[0].to(device)\n",
    "        targets = data[0].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}